<!DOCTYPE html>
<html>
<head>
  <title>VEU-Bench</title>
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <script src="https://kit.fontawesome.com/f8ddf9854a.js" crossorigin="anonymous"></script>
  <meta charset="utf-8">
  <meta name="description"
        content="VEU-Bench: Towards Comprehensive Understanding of Video Editing">
  <meta name="keywords" content="Video Editing Understanding, VideoLLM, Benchmark">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VEU-Bench: Towards Comprehensive Understanding of Video Editing</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">
  <script src="https://kit.fontawesome.com/fff5b27ec1.js" crossorigin="anonymous"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/table.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-item has-dropdown is-hoverable">
    <a class="navbar-link">
      More Research
    </a>
    <div class="navbar-dropdown">
      <!-- Update with your related projects or keep empty -->
      <a class="navbar-item" href="https://github.com/yongliang-wu/Repurpose">
        VideoRepurpose
      </a>
      <!-- Add more projects as needed -->
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">VEU-Bench: Towards Comprehensive Understanding of Video Editing</h1>

          <div class="is-size-4 publication-authors">
            <span class="author-block">
              <a href="https://www.linkedin.com/in/li-bozheng-612316285/">Bozheng Li</a><sup>1,2</sup><strong>*</strong>,
            </span>
            <span class="author-block">
              <a href="https://yongliang-wu.github.io/">Yongliang Wu</a><sup>1,3</sup><strong>‚Ä†</strong>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/yi-lu-tom/">Yi Lu</a><sup>1,4</sup><strong>‚Ä†</strong>,
            </span>
            <span class="author-block">
              <a href="#">Jiashuo Yu</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="#">Licheng Tang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Jiawang Cao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Wenqing Zhu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Yuyang Sun</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Jay Wu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Wenbo Zhu</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-6 mt-2">
            <span><strong>*</strong> = Core Contributor</span><br>
            <span><strong>‚Ä†</strong> = Equal Contribution</span>
          </div>

          <div class="is-size-5 publication-authors mt-3">
            <span class="author-block"><sup>1</sup>Opus AI Research,</span>
            <span class="author-block"><sup>2</sup>Brown University,</span>
            <span class="author-block"><sup>3</sup>Southeast University,</span>
            <span class="author-block"><sup>4</sup>University of Toronto,</span>
            <span class="author-block"><sup>5</sup>Fudan University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/your-paper-id"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- arXiv Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/your-paper-id"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/yourusername/veu-bench"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/yourusername/veu-bench/releases"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon" style="font-size:18px">
                    ü§ó
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">üîîNews</h2>
        <div class="content has-text-justified">
          <p>
            <b>üèÜ [2025-02-26]: Our paper was accepted to CVPR 2025 üéâ</b>
          </p>
          <p>
            <b>üî• [2025-04-04]: Our paper was selected as a <span style="color:#e76f51;">Highlight (Top 3‚Äì5%)</span> paper ‚ú®</b>
          </p>
      </div>
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified ">
          <p>
            Widely shared videos on the internet are often edited. Recently, although Video Large Language Models (Vid-LLMs) have made great progress in general video understanding tasks, their capabilities in video editing understanding (VEU) tasks remain unexplored.
            To address this gap, in this paper, we introduce <strong>VEU-Bench</strong> (<strong>V</strong>ideo <strong>E</strong>diting <strong>U</strong>nderstanding <strong>Bench</strong>mark), a comprehensive benchmark that categorizes video editing components across various dimensions, from intra-frame features like shot size to inter-shot attributes such as cut types and transitions.
            Unlike previous video editing understanding benchmarks that focus mainly on editing element classification, VEU-Bench encompasses <strong>19 fine-grained tasks</strong> across three stages: recognition, reasoning, and judging.
            To enhance the annotation of VEU automatically, we built an annotation pipeline integrated with an ontology-based knowledge base.
            Through extensive experiments with 11 state-of-the-art Vid-LLMs, our findings reveal that current Vid-LLMs face significant challenges in VEU tasks, with some performing worse than random choice.
            To alleviate this issue, we develop <strong>Oscars</strong><sup title="Named after the Academy Awards.">‚òÖ</sup>, a VEU expert model fine-tuned on the curated VEU-Bench dataset.
            It outperforms existing open-source Vid-LLMs on VEU-Bench by over <strong>28.3%</strong> in accuracy and achieves performance comparable to commercial models like GPT-4o.
            We also demonstrate that incorporating VEU data significantly enhances the performance of Vid-LLMs on general video understanding benchmarks, with an average improvement of <strong>8.3%</strong> across nine reasoning tasks.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>

<!-- DATASET SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 veu">
      <img src="static/images/favicon.ico" style="width:2em;vertical-align: middle" alt="Logo"/>
      <span class="veu">VEU-Bench</span>
    </h1>
  </div>
</section>



<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            We present <strong>VEU-Bench</strong> (Video Editing Understanding Benchmark), the first comprehensive benchmark designed to evaluate the video editing understanding capabilities of Video Large Language Models (Vid-LLMs). Unlike general video understanding tasks, video editing understanding (VEU) requires models to recognize abstract and symbolic editing elements‚Äîsuch as shot types, camera motions, cut types, and transitions‚Äîand to reason about their functions and stylistic intentions within narrative contexts.
          </p>

          <p>
            VEU-Bench introduces a <strong>three-level evaluation paradigm</strong>‚Äî<em>recognition</em>, <em>reasoning</em>, and <em>judging</em>‚Äîacross <strong>10 editing dimensions</strong> including both intra-frame (e.g., shot size, angle, color), intra-shot (e.g., motion, speed), and inter-shot elements (e.g., cut type, transition). With over <strong>50K high-quality QA pairs</strong> grounded in real edited videos, VEU-Bench offers a rich and diverse benchmark to evaluate models' ability to perceive visual editing cues, explain changes, and interpret artistic intentions.
          </p>
          
          <div class="content has-text-centered">
          <img src="static/images/teaser.png" alt="VEU Editing Dimensions" class="center" width="80%">
          </div>

          <p>
            To generate high-quality annotations, we design an <strong>ontology-based annotation pipeline</strong> built upon domain-specific knowledge extracted from professional video editing tutorials. This system rewrites abstract editing concepts into video-specific prompts and explanations, enabling scalable generation of reasoning and judging tasks with minimal human intervention.
          </p>

          <div class="content has-text-centered">
          <img src="static/images/data_pipeline.png" alt="VEU Editing Dimensions" class="center" width="80%">
          </div>

          <p>
            Through extensive evaluations, we reveal that current state-of-the-art Vid-LLMs perform poorly on VEU tasks‚Äîoften worse than random guessing in some categories‚Äîdue to their weak alignment between editing knowledge and visual perception. To address this, we introduce <strong>Oscars</strong>, a VEU expert model fine-tuned on VEU-Bench. Oscars achieves a <strong>28.3% performance gain</strong> over existing open-source models and even rivals commercial models like GPT-4o.
          </p>

          <p>
            More importantly, we demonstrate that <strong>training on VEU-Bench can significantly improve Vid-LLMs on general video reasoning tasks</strong>, with an average boost of 8.3% across multiple benchmarks. These findings highlight VEU-Bench as not only a challenge for editing-specific evaluation but also a valuable dataset for enhancing abstract reasoning in video foundation models.
          </p>
        </div>
      </div>
    </div>
  
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Comparisons with Existing Benchmarks</h2>
        <div class="content has-text-justified">
          <p>
            <div class="content has-text-justified">
              <p>
                While previous VEU benchmarks primarily focus on basic recognition tasks, <em>VEU-Bench</em> extends the evaluation to include both <strong>reasoning</strong> and <strong>judging</strong>.
              </p>
            
              <p>
                <strong>Recognition:</strong> Models classify editing elements across 10 dimensions through multiple-choice questions.
              </p>
            
              <p>
                <strong>Reasoning:</strong> Models explain changes in editing elements (e.g., shot size, transitions) with supporting evidence.
              </p>
            
              <p>
                <strong>Judging:</strong> Models assess the purpose and impact of editing choices, demonstrating an understanding of the creator's intent and storytelling effects.
              </p>
            
              <p>
                This three-level design offers a more comprehensive and realistic assessment of video editing understanding compared to earlier benchmarks.
              </p>
            </div>
            
          </p>
          <div class="content has-text-centered">
            <img src="static/images/compare.png" alt="comparison" class="center">
            <p> Comparison between VEU-Bench and previous
              VEU benchmarks. VEU-Bench encompasses a wider range of video editing components and includes high-level reasoning and judgment tasks.</p>
          </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Statistics</h2>
        <div class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/statistics/answer_length.png" alt="answer length" width="100%"/>
              <p> 
                Answer length statistics of the curated VEU-Bench Dataset. The answer length ranges from 0-115 characters, with the majority between 31-50 characters.
              </p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/statistics/duration.png" alt="video duration" width="100%"/>
              <p> 
                Video duration statistics of the curated VEU-Bench Dataset. The video durations range from 1 to over 60 seconds, with the majority between 1 and 12 seconds.
              </p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/statistics/portion.png" alt="datasource portion" width="82%"/>
              <p> 
                Sampled video sources in our VEU-Bench dataset from various domains. The majority of the videos are cherry-picked from the AVE dataset, with the rest from MovieCuts and AutoTransition.  
              </p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/statistics/task_portion.png" alt="task portion" width="72%"/>
              <p> 
                Task portion statistics of our dataset. The majority of the task was perception, followed by reasoning and judge.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  
  </div>
</section>


<!-- RESULTS SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 veu">Experiment Results</h1>
  </div>
</section>

<section class="section">
  <div class="container">
    <!-------------------------------------------------------------------- RESULTS SECTION -------------------------------------------------------------------->
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3" id="leaderboard">Leaderboard</h2>
        <div class="content has-text-justified">
          <p>
            We evaluate various models including commercial models and open-source ones. Current Vid-LLMs exhibit poor performance across all benchmark dimensions, while our expert model Oscars exhibits improvements across all dimensions compared to the baseline: Qwen2-VL.
          </p>
        </div>
        <br>
        <div class="model-labels-container">
          <span class="leaderboard-label ours">Ours</span>
          <span class="leaderboard-label open_source">Open-Source</span>
          <span class="leaderboard-label commercial">Commercial</span>
        </div>
        <br>
        <div class="content has-text-centered">
          <p>
            Click on Recognition, Reasoning&Judging or All Score to expand detailed results.
          </p>
        </div>
        <div class="leaderboard-container">
          <div class="table-wrapper">
            <table id="veu-table">
              <thead>
                <tr>
                  <th colspan="3" class="reset-cell clickable" style="text-align: center;">Reset</th>
                  <th class="recog-details-cell clickable" colspan="1">Recognition</th>
                  <th class="rnj-details-cell clickable" colspan="1">Reasoning&Judging</th>
                  <th class="all-details-cell clickable" colspan="1">All Score</th>
                </tr>
                <tr>
                  <th class="sortable clickable" data-sort="string">Name</th>
                  <th class="clickable" data-sort="string">Model Size</th>
                  <th class="clickable" data-sort="string">Frame Numbers</th>
                  <th class="sortable clickable recog-overall" data-sort="number">Overall</th>
                  <th class="hidden recog-details sortable clickable" data-sort="number">Shot Subject</th>
                  <th class="hidden recog-details sortable clickable" data-sort="number">Shot Color</th>
                  <th class="hidden recog-details sortable clickable" data-sort="number">Shot Size</th>
                  <th class="hidden recog-details sortable clickable" data-sort="number">Shot Angle</th>
                  <th class="hidden recog-details sortable clickable" data-sort="number">Shot Location</th>
                  <th class="hidden recog-details sortable clickable" data-sort="number">Shot Type</th>
                  <th class="hidden recog-details sortable clickable" data-sort="number">Shot Motion</th>
                  <th class="hidden recog-details sortable clickable" data-sort="number">Shot Speed</th>
                  <th class="hidden recog-details sortable clickable" data-sort="number">Transition</th>
                  <th class="hidden recog-details sortable clickable" data-sort="number">Cut Type</th>
                
                  <th class="sortable clickable rnj-overall" data-sort="number">Overall</th>
                  <th class="hidden rnj-details sortable clickable" data-sort="number">Shot Size</th>
                  <th class="hidden rnj-details sortable clickable" data-sort="number">Shot Angle</th>
                  <th class="hidden rnj-details sortable clickable" data-sort="number">Shot Location</th>
                  <th class="hidden rnj-details sortable clickable" data-sort="number">Shot Type</th>
                  <th class="hidden rnj-details sortable clickable" data-sort="number">Shot Motion</th>
                  <th class="hidden rnj-details sortable clickable" data-sort="number">Transition</th>
                  <th class="hidden rnj-details sortable clickable" data-sort="number">Cut Type</th>
                  <th class="sortable clickable all-overall" data-sort="number">Overall</th>
                </tr>
                
              </thead>
              <tbody>
                <!-- Table body will be populated dynamically -->
              </tbody>
            </table>
            <p class="test-desc"> Overall results of different models on the VEU-Bench leaderboard.</p> <p>The best-performing model in each category is <b>in-bold</b>, and the second best is <u>underlined</u>.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- DEMO SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 veu">Data & Answer Demonstration</h1>
  </div>
</section>

<section class="section"></section>
  <!-------------------------------------------------------------------- Recog EXAMPLES SECTION ---------------------------------------------------------------->
  <div class="container"></div>
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
      <h2 class="title is-3" id="examples">Recognition Task Examples</h2>
      <div class="carousel results-carousel">
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/demos/rec/0.png" alt="grade-lv" height="150%"/>
          </div>
        </div>
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/demos/rec/1.png" alt="grade-lv" height="150%"/>
          </div>
        </div>
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/demos/rec/2.png" alt="grade-lv" height="150%"/>
          </div>
        </div>
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/demos/rec/3.png" alt="grade-lv" height="150%"/>
          </div>
        </div>
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/demos/rec/4.png" alt="grade-lv" width="150%"/>
          </div>
        </div>
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/demos/rec/5.png" alt="grade-lv" width="150%"/>
          </div>
        </div>
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/demos/rec/6.png" alt="grade-lv" width="150%"/>
          </div>
        </div>
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/demos/rec/7.png" alt="grade-lv" width="150%"/>
          </div>
        </div>
      </div>
    </div>
  </div>

  <!-------------------------------------------------------------------- Reason & Judge EXAMPLES SECTION ------------------------------------------------------->
  <div class="container"></div>
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
      <h2 class="title is-3" id="examples">Reasoning & Judge Task Examples</h2>
      <div class="carousel results-carousel">
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/demos/rnj/0.png" alt="grade-lv" width="150%"/>
          </div>
        </div>
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/demos/rnj/1.png" alt="grade-lv" width="150%"/>
          </div>
        </div>
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/demos/rnj/2.png" alt="grade-lv" width="150%"/>
          </div>
        </div>
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/demos/rnj/3.png" alt="grade-lv" width="150%"/>
          </div>
        </div>
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/demos/rnj/4.png" alt="grade-lv" width="150%"/>
          </div>
        </div>
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/demos/rnj/5.png" alt="grade-lv" width="150%"/>
          </div>
        </div>
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/demos/rnj/6.png" alt="grade-lv" width="150%"/>
          </div>
        </div>
      </div>
    </div>
  </div>

</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
    @article{li_2025_veubench,
      author = {Li, Bozheng and Wu, Yongliang and Lu, Yi and Yu, Jiashuo and Tang, Licheng and Cao, Jiawang and Zhu, Wenqing and Sun, Yuyang and Wu, Jay and Zhu, Wenbo},
      month = {03},
      title = {VEU-Bench: Towards Comprehensive Understanding of Video Editing},
      journal = {CVPR},
      url = {https://cvpr.thecvf.com/virtual/2025/poster/34180},
      year = {2025},
      urldate = {2025-04-16},
    }
  </code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>

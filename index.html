<!DOCTYPE html>
<html>
<head>
  <title>VEU-Bench</title>
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <script src="https://kit.fontawesome.com/f8ddf9854a.js" crossorigin="anonymous"></script>
  <meta charset="utf-8">
  <meta name="description"
        content="VEU-Bench: Towards Comprehensive Understanding of Video Editing">
  <meta name="keywords" content="Video Editing Understanding, VideoLLM, Benchmark">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VEU-Bench: Towards Comprehensive Understanding of Video Editing</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">
  <script src="https://kit.fontawesome.com/fff5b27ec1.js" crossorigin="anonymous"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/table.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-item has-dropdown is-hoverable">
    <a class="navbar-link">
      More Research
    </a>
    <div class="navbar-dropdown">
      <!-- Update with your related projects or keep empty -->
      <a class="navbar-item" href="https://github.com/yongliang-wu/Repurpose">
        VideoRepurpose
      </a>
      <!-- Add more projects as needed -->
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">VEU-Bench: Towards Comprehensive Understanding of Video Editing</h1>

          <div class="is-size-4 publication-authors">
            <span class="author-block">
              <a href="https://www.linkedin.com/in/li-bozheng-612316285/">Bozheng Li</a><sup>1,2</sup><strong>*</strong>,
            </span>
            <span class="author-block">
              <a href="https://yongliang-wu.github.io/">Yongliang Wu</a><sup>1,3</sup><strong>‚Ä†</strong>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/yi-lu-tom/">Yi Lu</a><sup>1,4</sup><strong>‚Ä†</strong>,
            </span>
            <span class="author-block">
              <a href="#">Jiashuo Yu</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="#">Licheng Tang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Jiawang Cao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Wenqing Zhu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Yuyang Sun</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Jay Wu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Wenbo Zhu</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-6 mt-2">
            <span><strong>*</strong> = Core Contributor</span><br>
            <span><strong>‚Ä†</strong> = Equal Contribution</span>
          </div>

          <div class="is-size-5 publication-authors mt-3">
            <span class="author-block"><sup>1</sup>Opus AI Research,</span>
            <span class="author-block"><sup>2</sup>Brown University,</span>
            <span class="author-block"><sup>3</sup>Southeast University,</span>
            <span class="author-block"><sup>4</sup>University of Toronto,</span>
            <span class="author-block"><sup>5</sup>Fudan University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/your-paper-id"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- arXiv Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/your-paper-id"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/yourusername/veu-bench"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/yourusername/veu-bench/releases"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon" style="font-size:18px">
                    ü§ó
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">üîîNews</h2>
        <div class="content has-text-justified">
          <p>
            <b>üèÜ [2025-02-26]: Our paper was accepted to CVPR 2025 üéâ</b>
          </p>
          <p>
            <b>üî• [2025-04-04]: Our paper was selected as a <span style="color:#e76f51;">Highlight (Top 3‚Äì5%)</span> paper ‚ú®</b>
          </p>
      </div>
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified ">
          <p>
            Widely shared videos on the internet are often edited. Recently, although Video Large Language Models (Vid-LLMs) have made great progress in general video understanding tasks, their capabilities in video editing understanding (VEU) tasks remain unexplored.
            To address this gap, in this paper, we introduce <strong>VEU-Bench</strong> (<strong>V</strong>ideo <strong>E</strong>diting <strong>U</strong>nderstanding <strong>Bench</strong>mark), a comprehensive benchmark that categorizes video editing components across various dimensions, from intra-frame features like shot size to inter-shot attributes such as cut types and transitions.
            Unlike previous video editing understanding benchmarks that focus mainly on editing element classification, VEU-Bench encompasses <strong>19 fine-grained tasks</strong> across three stages: recognition, reasoning, and judging.
            To enhance the annotation of VEU automatically, we built an annotation pipeline integrated with an ontology-based knowledge base.
            Through extensive experiments with 11 state-of-the-art Vid-LLMs, our findings reveal that current Vid-LLMs face significant challenges in VEU tasks, with some performing worse than random choice.
            To alleviate this issue, we develop <strong>Oscars</strong><sup title="Named after the Academy Awards.">‚òÖ</sup>, a VEU expert model fine-tuned on the curated VEU-Bench dataset.
            It outperforms existing open-source Vid-LLMs on VEU-Bench by over <strong>28.3%</strong> in accuracy and achieves performance comparable to commercial models like GPT-4o.
            We also demonstrate that incorporating VEU data significantly enhances the performance of Vid-LLMs on general video understanding benchmarks, with an average improvement of <strong>8.3%</strong> across nine reasoning tasks.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>

<!-- DATASET SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 veu">
      <img src="static/images/favicon.ico" style="width:2em;vertical-align: middle" alt="Logo"/>
      <span class="mmmu">VEU-Bench</span>
    </h1>
  </div>
</section>



<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            We present <strong>VEU-Bench</strong> (Video Editing Understanding Benchmark), the first comprehensive benchmark designed to evaluate the video editing understanding capabilities of Video Large Language Models (Vid-LLMs). Unlike general video understanding tasks, video editing understanding (VEU) requires models to recognize abstract and symbolic editing elements‚Äîsuch as shot types, camera motions, cut types, and transitions‚Äîand to reason about their functions and stylistic intentions within narrative contexts.
          </p>

          <p>
            VEU-Bench introduces a <strong>three-level evaluation paradigm</strong>‚Äî<em>recognition</em>, <em>reasoning</em>, and <em>judging</em>‚Äîacross <strong>10 editing dimensions</strong> including both intra-frame (e.g., shot size, angle, color), intra-shot (e.g., motion, speed), and inter-shot elements (e.g., cut type, transition). With over <strong>50K high-quality QA pairs</strong> grounded in real edited videos, VEU-Bench offers a rich and diverse benchmark to evaluate models' ability to perceive visual editing cues, explain changes, and interpret artistic intentions.
          </p>
          
          <div class="content has-text-centered">
          <img src="static/images/teaser.png" alt="VEU Editing Dimensions" class="center" width="80%">
          </div>

          <p>
            To generate high-quality annotations, we design an <strong>ontology-based annotation pipeline</strong> built upon domain-specific knowledge extracted from professional video editing tutorials. This system rewrites abstract editing concepts into video-specific prompts and explanations, enabling scalable generation of reasoning and judging tasks with minimal human intervention.
          </p>

          <div class="content has-text-centered">
          <img src="static/images/data_pipeline.png" alt="VEU Editing Dimensions" class="center" width="80%">
          </div>

          <p>
            Through extensive evaluations, we reveal that current state-of-the-art Vid-LLMs perform poorly on VEU tasks‚Äîoften worse than random guessing in some categories‚Äîdue to their weak alignment between editing knowledge and visual perception. To address this, we introduce <strong>Oscars</strong>, a VEU expert model fine-tuned on VEU-Bench. Oscars achieves a <strong>28.3% performance gain</strong> over existing open-source models and even rivals commercial models like GPT-4o.
          </p>

          <p>
            More importantly, we demonstrate that <strong>training on VEU-Bench can significantly improve Vid-LLMs on general video reasoning tasks</strong>, with an average boost of 8.3% across multiple benchmarks. These findings highlight VEU-Bench as not only a challenge for editing-specific evaluation but also a valuable dataset for enhancing abstract reasoning in video foundation models.
          </p>
        </div>
      </div>
    </div>
  
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Comparisons with Existing Benchmarks</h2>
        <div class="content has-text-justified">
          <p>
            <div class="content has-text-justified">
              <p>
                While previous VEU benchmarks primarily focus on basic recognition tasks, <em>VEU-Bench</em> extends the evaluation to include both <strong>reasoning</strong> and <strong>judging</strong>.
              </p>
            
              <p>
                <strong>Recognition:</strong> Models classify editing elements across 10 dimensions through multiple-choice questions.
              </p>
            
              <p>
                <strong>Reasoning:</strong> Models explain changes in editing elements (e.g., shot size, transitions) with supporting evidence.
              </p>
            
              <p>
                <strong>Judging:</strong> Models assess the purpose and impact of editing choices, demonstrating an understanding of the creator's intent and storytelling effects.
              </p>
            
              <p>
                This three-level design offers a more comprehensive and realistic assessment of video editing understanding compared to earlier benchmarks.
              </p>
            </div>
            
          </p>
          <div class="content has-text-centered">
            <img src="static/images/compare.png" alt="comparison" class="center">
            <p> Comparison between VEU-Bench and previous
              VEU benchmarks. VEU-Bench encompasses a wider range of video editing components and includes high-level reasoning and judgment tasks.</p>
          </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Statistics</h2>
        <div class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/statistics/answer_length.png" alt="answer length" width="100%"/>
              <p> 
                Answer length statistics of the curated VEU-Bench Dataset. The answer length ranges from 0-115 characters, with the majority between 31-50 characters.
              </p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/statistics/duration.png" alt="video duration" width="100%"/>
              <p> 
                Video duration statistics of the curated VEU-Bench Dataset. The video durations range from 1 to over 60 seconds, with the majority between 1 and 12 seconds.
              </p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/statistics/portion.png" alt="datasource portion" width="82%"/>
              <p> 
                Sampled video sources in our VEU-Bench dataset from various domains. The majority of the videos are cherry-picked from the AVE dataset, with the rest from MovieCuts and AutoTransition.  
              </p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/statistics/task_portion.png" alt="task portion" width="72%"/>
              <p> 
                Task portion statistics of our dataset. The majority of the task was perception, followed by reasoning and judge.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  
  </div>
</section>


<!-- RESULTS SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 mmmu">Experiment Results</h1>
  </div>
</section>

<section class="section">
  <div class="container">
    <!-------------------------------------------------------------------- RESULTS SECTION -------------------------------------------------------------------->
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3" id="leaderboard">Leaderboard</h2>
        <div class="content has-text-justified">
          <p>
            We evaluate various models including LLMs and LMMs.
            In each type, we consider both closed- and open-source models.
            Our evaluation is conducted under a zero-shot setting to assess the capability of models to generate accurate answers without fine-tuning or few-shot demonstrations on our benchmark.
            For all models, we use the default prompt provided by each model for multi-choice or open QA, if available.
            If models do not provide prompts for task types in MMMU, we conduct prompt engineering on the validation set and use the most effective prompt for the later zero-shot experiment.
          </p>
        </div>
        <br>
        <div class="model-labels-container">
          <span class="leaderboard-label human_expert">Human Expert</span>
          <span class="leaderboard-label open_source">Open-Source</span>
          <span class="leaderboard-label proprietary">Proprietary</span>
        </div>
        <br>
        <div class="content has-text-centered">
          <p>
            Click on MMMU-Pro, MMMU (Val) or MMMU (Test) to expand detailed results.
          </p>
        </div>
        <div class="leaderboard-container">
          <div class="table-wrapper">
            <table id="veu-table">
              <thead>
                <tr>
                  <th colspan="3" class="reset-cell clickable" style="text-align: center;">Reset</th>
                  <th class="pro-details-cell clickable" colspan="1">Recognition</th>
                  <th class="val-details-cell clickable" colspan="1">Reasoning</th>
                  <th class="test-details-cell clickable" colspan="1">Judging</th>
                </tr>
                <tr>
                  <th class="sortable clickable" data-sort="string">Name</th>
                  <th class="clickable" data-sort="string">Parameter Size</th>
                  <th class="clickable" data-sort="string">FPS</th>
                  <th class="sortable clickable pro-overall" data-sort="number">Overall</th>
                  <th class="hidden pro-details sortable clickable" data-sort="number">Vision</th>
                  <th class="hidden pro-details sortable clickable" data-sort="number">Standard</th>
                  <th class="sortable clickable val-overall" data-sort="number">Overall</th>
                  <th class="hidden val-details sortable clickable" data-sort="number">Art & Design</th>
                  <th class="hidden val-details sortable clickable" data-sort="number">Business</th>
                  <th class="hidden val-details sortable clickable" data-sort="number">Science</th>
                  <th class="hidden val-details sortable clickable" data-sort="number">Health & Medicine</th>
                  <th class="hidden val-details sortable clickable" data-sort="number">Human. & Social Sci.</th>
                  <th class="hidden val-details sortable clickable" data-sort="number">Tech & Eng.</th>
                  <th class="sortable clickable test-overall" data-sort="number">Overall</th>
                  <th class="hidden test-details sortable clickable" data-sort="number">Art & Design</th>
                  <th class="hidden test-details sortable clickable" data-sort="number">Business</th>
                  <th class="hidden test-details sortable clickable" data-sort="number">Science</th>
                  <th class="hidden test-details sortable clickable" data-sort="number">Health & Medicine</th>
                  <th class="hidden test-details sortable clickable" data-sort="number">Human. & Social Sci.</th>
                  <th class="hidden test-details sortable clickable" data-sort="number">Tech & Eng.</th>
                </tr>
              </thead>
              <tbody>
                <!-- Table body will be populated dynamically -->
              </tbody>
            </table>
            <p class="test-desc"> Overall results of different models on the MMMU leaderboard. The best-performing model in each category is <b>in-bold</b>, and the second best is <u>underlined</u>. *: results provided by the authors.</p>
          </div>
        </div>
      </div>
    </div>
  </section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {CVPR},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
